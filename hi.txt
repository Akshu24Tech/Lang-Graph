It is great to see the skeleton you have built! You have successfully combined the **State**, **Nodes**, and **Conditional Edges** logic from the first four videos.

To make this a professional project, we should move away from a single script and organize it into a modular structure. This allows you to scale the project as you add more tools or agents.

### Project Structure

Based on the lessons from **CampusX Video 5**, here is the recommended file structure:

```text
reflective_coder/
├── .env                # Your API Keys (OpenAI/Gemini)
├── requirements.txt    # dependencies (langgraph, langchain-openai, etc.)
├── state.py            # Definition of the AssistantState
├── nodes.py            # Logic for Coder and Executor
└── main.py             # Graph construction and execution

```

---

### 1. `state.py` (The "Shared Memory")

As learned in **Video 6 & 8**, we use `Annotated` and `operator.add` to ensure our error history doesn't get overwritten.

```python
from typing import TypedDict, Annotated, List, Optional
import operator

class AssistantState(TypedDict):
    task: str
    # Video 6 Lesson: Reducers allow us to append to the list rather than overwrite
    code_history: Annotated[List[str], operator.add]
    error_history: Annotated[List[str], operator.add]
    iterations: Annotated[int, operator.add]
    max_iterations: int

```

---

### 2. `nodes.py` (The "Workers")

Here we implement the **LLM logic** (from Video 8) and **Actual Python Execution**.

```python
import sys
import io
from langchain_openai import ChatOpenAI # or Gemini
from langchain_core.messages import SystemMessage, HumanMessage
from state import AssistantState

llm = ChatOpenAI(model="gpt-4o-mini") # Video 8: Different models for different tasks

def coder_node(state: AssistantState):
    """Generates or fixes code based on history."""
    print(f"--- CODER: Attempt {state['iterations'] + 1} ---")
    
    # Use History to inform the LLM
    last_error = state["error_history"][-1] if state["error_history"] else "No errors yet."
    
    prompt = [
        SystemMessage(content="You are an expert Python developer. Return ONLY valid python code without markdown blocks."),
        HumanMessage(content=f"Task: {state['task']}\nLast Error: {last_error}\nFix the code.")
    ]
    
    response = llm.invoke(prompt)
    return {"code_history": [response.content], "iterations": 1}

def executor_node(state: AssistantState):
    """Actually executes the code and captures output or errors."""
    print("--- EXECUTOR: Running Code ---")
    code = state["code_history"][-1]
    
    # Redirect stdout to capture print statements
    old_stdout = sys.stdout
    new_stdout = io.StringIO()
    sys.stdout = new_stdout
    
    try:
        exec(code)
        result = new_stdout.getvalue()
        sys.stdout = old_stdout
        return {"error_history": ["SUCCESS"]} # Signal for the router
    except Exception as e:
        sys.stdout = old_stdout
        return {"error_history": [str(e)]}

def router_logic(state: AssistantState):
    """Video 7: Conditional logic to stop or loop."""
    if state["error_history"][-1] == "SUCCESS":
        return "end"
    if state["iterations"] >= state["max_iterations"]:
        return "end"
    return "retry"

```

---

### 3. `main.py` (The "Orchestrator")

This is where you "wire" the graph together using the **Iterative Pattern** from **Video 8**.

```python
from langgraph.graph import StateGraph, END
from state import AssistantState
from nodes import coder_node, executor_node, router_logic

# 1. Initialize Graph
workflow = StateGraph(AssistantState)

# 2. Add Nodes
workflow.add_node("coder", coder_node)
workflow.add_node("executor", executor_node)

# 3. Define Flow
workflow.set_entry_point("coder")
workflow.add_edge("coder", "executor")

# Video 7 & 8: Conditional looping
workflow.add_conditional_edges(
    "executor",
    router_logic,
    {
        "retry": "coder",
        "end": END
    }
)

# 4. Compile
app = workflow.compile()

if __name__ == "__main__":
    config = {"task": "Write a function to find the 10th fibonacci number", 
              "max_iterations": 3, 
              "iterations": 0}
    
    final_state = app.invoke(config)
    print("\n--- FINAL CODE ---")
    print(final_state["code_history"][-1])

```

---

### Summary of Video Applications

* **Video 4 & 5:** Used `StateGraph` and nodes as simple functions.
* **Video 6:** Used `Annotated` with `operator.add` to keep the "Error History" list.
* **Video 7:** Implemented `add_conditional_edges` and a `router_logic` function.
* **Video 8:** Established the `max_iterations` safety guard and version tracking.

**Would you like me to show you how to add a "Human-in-the-Loop" step so you can approve the code before it executes?**